# 激活函数介绍：ReLU 变体及其他激活函数

## 一、ReLU 及其变体

### 1. ReLU (Rectified Linear Unit)

$$
f(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
$$

**特点：**
- **优点**：计算简单、缓解梯度消失、具有稀疏性
- **缺点**：存在"神经元死亡"问题（输出恒为 0）、负值区域梯度为 0

**编程实现：**

```python
import numpy as np

def relu(x):
    return np.maximum(0, x)

# PyTorch: torch.nn.ReLU() 或 torch.relu(x)
# TensorFlow: tf.nn.relu(x)
```

---

### 2. Leaky ReLU

$$
f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}
$$

**特点：**
- \(\alpha\) 通常取 0.01
- 负值区域有非零梯度，缓解"神经元死亡"问题
- 保持稀疏性

**编程实现：**

```python
import numpy as np

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

# PyTorch: torch.nn.LeakyReLU(negative_slope=0.01)
# TensorFlow: tf.nn.leaky_relu(x, alpha=0.01)
```

---

### 3. PReLU (Parametric ReLU)

$$
f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}
$$

**特点：**
- \(\alpha\) 为可学习参数，每个通道可不同
- 比 Leaky ReLU 更灵活，表达能力更强

**编程实现：**

```python
import numpy as np

def prelu(x, alpha):
    """alpha 为可学习参数，形状需与 x 对应（如每通道一个）"""
    return np.where(x > 0, x, alpha * x)

# PyTorch: torch.nn.PReLU(num_parameters=1)  # num_parameters 可为通道数
# 使用: prelu = nn.PReLU(); output = prelu(x)
```

---

### 4. ELU (Exponential Linear Unit)

$$
f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{if } x \leq 0 \end{cases}
$$

**特点：**
- \(\alpha\) 通常取 1.0
- 负值区域平滑，均值更接近 0
- 收敛往往更快，但计算稍复杂

**编程实现：**

```python
import numpy as np

def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

# PyTorch: torch.nn.ELU(alpha=1.0)
# TensorFlow: tf.nn.elu(x)
```

---

### 5. SELU (Scaled ELU)

$$
f(x) = \lambda \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{if } x \leq 0 \end{cases}
$$

**特点：**
- \(\lambda \approx 1.05\)，\(\alpha \approx 1.67\)
- 在特定条件下具有自归一化性质
- 常用于较深的网络

**编程实现：**

```python
import numpy as np

def selu(x, lambda_=1.0507, alpha=1.67326):
    return lambda_ * np.where(x > 0, x, alpha * (np.exp(x) - 1))

# PyTorch: torch.nn.SELU()
# TensorFlow: tf.nn.selu(x)
```

---

### 6. GELU (Gaussian Error Linear Unit)

$$
f(x) = x \cdot \Phi(x)
$$

**特点：**
- \(\Phi(x)\) 为标准正态分布的累积分布函数
- 常用于 Transformer（如 BERT、GPT）
- 对输入做"软门控"，更平滑

**编程实现：**

```python
import numpy as np
from scipy.special import erf

def gelu(x):
    """精确形式: x * Φ(x)，Φ 为标准正态 CDF"""
    return 0.5 * x * (1 + erf(x / np.sqrt(2)))

def gelu_approx(x):
    """近似形式，常用于实践中"""
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))

# PyTorch: torch.nn.GELU()
# TensorFlow: tf.nn.gelu(x)
```

---

### 7. Swish / SiLU

$$
f(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

**特点：**
- 非单调，负值区域有小的负输出
- 在不少任务上优于 ReLU
- 计算量略大

**编程实现：**

```python
import numpy as np

def swish(x):
    return x / (1 + np.exp(-x))

# 等价于 x * sigmoid(x)
def silu(x):
    return x * (1 / (1 + np.exp(-x)))

# PyTorch: torch.nn.SiLU() 或 torch.nn.functional.silu(x)
# TensorFlow: tf.nn.silu(x)
```

---

### 8. Mish

$$
f(x) = x \cdot \tanh(\ln(1 + e^x))
$$

**特点：**
- 平滑、无上界、非单调
- 常用于目标检测等任务

**编程实现：**

```python
import numpy as np

def mish(x):
    return x * np.tanh(np.log1p(np.exp(x)))

# 数值稳定版本（避免大 x 时溢出）
def mish_stable(x):
    return x * np.tanh(np.log(1 + np.exp(-np.abs(x))) + np.maximum(0, x))

# PyTorch: torch.nn.Mish()
# TensorFlow: 需自定义或使用 tfa.activations.mish
```

---

## 二、其他常见激活函数

### 1. Sigmoid

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**特点：**
- 输出在 (0, 1)，常用于二分类输出层
- 易饱和，梯度消失明显，一般不用在隐藏层

**编程实现：**

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 数值稳定版本
def sigmoid_stable(x):
    x = np.clip(x, -500, 500)  # 防止溢出
    return 1 / (1 + np.exp(-x))

# PyTorch: torch.nn.Sigmoid() 或 torch.sigmoid(x)
# TensorFlow: tf.nn.sigmoid(x)
```

---

### 2. Tanh (双曲正切)

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

**特点：**
- 输出在 (-1, 1)，零中心
- 仍存在饱和和梯度消失，但比 Sigmoid 好

**编程实现：**

```python
import numpy as np

def tanh(x):
    return np.tanh(x)
    # 或: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

# PyTorch: torch.nn.Tanh() 或 torch.tanh(x)
# TensorFlow: tf.nn.tanh(x)
```

---

### 3. Softmax

$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$

**特点：**
- 输出和为 1，常用于多分类输出层

**编程实现：**

```python
import numpy as np

def softmax(x, axis=-1):
    # 数值稳定：减去最大值防止溢出
    x_max = np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x - x_max)
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# PyTorch: torch.nn.Softmax(dim=-1) 或 torch.softmax(x, dim=-1)
# TensorFlow: tf.nn.softmax(x, axis=-1)
```

---

### 4. Maxout

$$
f(x) = \max(w_1^T x + b_1, w_2^T x + b_2, \ldots)
$$

**特点：**
- 可近似 ReLU 等分段线性函数
- 参数和计算量较大

**编程实现：**

```python
import numpy as np

def maxout(x, W, b):
    """
    x: 输入 (batch, in_features)
    W: 权重 (in_features, out_features, k)  # k 为组数
    b: 偏置 (out_features, k)
    返回每组的最大值
    """
    linear = np.dot(x, W) + b  # (batch, out_features, k)
    return np.max(linear, axis=-1)

# PyTorch: 无内置，需自定义层
# class Maxout(nn.Module):
#     def __init__(self, in_features, out_features, k):
#         super().__init__()
#         self.linear = nn.Linear(in_features, out_features * k)
#         self.k = k
#     def forward(self, x):
#         out = self.linear(x)
#         return out.view(*out.shape[:-1], -1, self.k).max(-1)[0]
```

---

## 三、对比总结

| 激活函数 | 优点 | 缺点 |
|---------|------|------|
| ReLU | 简单、稀疏、缓解梯度消失 | 易神经元死亡 |
| Leaky ReLU | 缓解死亡神经元 | 需设定 \(\alpha\) |
| PReLU | 可学习负斜率 | 参数更多 |
| ELU | 负值平滑、均值接近 0 | 计算稍慢 |
| GELU | 适合 Transformer | 计算较慢 |
| Swish | 性能好 | 计算较慢 |
| Sigmoid | 输出有界 | 易饱和、梯度消失 |
| Tanh | 零中心 | 仍有饱和 |

---

## 四、选择建议

- **隐藏层**：ReLU、Leaky ReLU、GELU、Swish 等
- **输出层**：二分类用 Sigmoid，多分类用 Softmax
- **Transformer**：常用 GELU
- **目标检测**：常用 Mish、Swish
