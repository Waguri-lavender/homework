# 激活函数介绍

## 1. ReLU 及其变体

### 1.1 ReLU (Rectified Linear Unit)
**公式**: $f(x) = \max(0, x)$

**特点**:
- 计算简单，梯度计算高效
- 解决梯度消失问题（在正区间梯度恒为 1）
- 缺点：存在"死神经元"问题（负值输出为 0，梯度为 0）

**实现**:
```python
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)
```

### 1.2 Leaky ReLU
**公式**: $f(x) = \max(\alpha x, x)$，其中 $\alpha$ 通常为 0.01

**特点**:
- 解决 ReLU 的"死神经元"问题
- 在负区间给予小的梯度（$\alpha$），避免神经元完全死亡
- 保留了 ReLU 在正区间的优势

**实现**:
```python
def leaky_relu(x, alpha=0.01):
    return np.maximum(alpha * x, x)

def leaky_relu_derivative(x, alpha=0.01):
    return np.where(x > 0, 1, alpha)
```

### 1.3 PReLU (Parametric ReLU)
**公式**: $f(x) = \max(\alpha x, x)$，其中 $\alpha$ 是可学习参数

**特点**:
- Leaky ReLU 的改进版本
- $\alpha$ 作为模型参数，在训练过程中自动学习最优值
- 比 Leaky ReLU 更灵活，但增加了参数量

### 1.4 ELU (Exponential Linear Unit)
**公式**: 
$$f(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha(e^x - 1) & \text{if } x \leq 0
\end{cases}$$

**特点**:
- 负区间平滑，输出均值接近 0
- 相比 ReLU 有更好的归一化特性
- 计算成本略高于 ReLU

**实现**:
```python
def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

def elu_derivative(x, alpha=1.0):
    return np.where(x > 0, 1, elu(x, alpha) + alpha)
```

### 1.5 GELU (Gaussian Error Linear Unit)
**公式**: $f(x) = x \cdot \Phi(x)$，其中 $\Phi(x)$ 是标准正态分布的累积分布函数

**特点**:
- 在 Transformer 等模型中广泛使用
- 平滑且非单调
- 近似公式: $f(x) \approx 0.5x(1 + \tanh(\sqrt{2/\pi}(x + 0.044715x^3)))$

**实现**:
```python
def gelu(x):
    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))

def gelu_derivative(x):
    # 简化版本，实际导数较复杂
    return 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3))) + \
           0.5 * x * (1 - np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3))**2) * \
           np.sqrt(2 / np.pi) * (1 + 3 * 0.044715 * x**2)
```

### 1.6 Swish
**公式**: $f(x) = x \cdot \sigma(x) = x \cdot \frac{1}{1 + e^{-x}}$

**特点**:
- Google 提出的激活函数
- 平滑、非单调
- 在某些任务上表现优于 ReLU

**实现**:
```python
def swish(x):
    return x * (1 / (1 + np.exp(-x)))

def swish_derivative(x):
    sigmoid_x = 1 / (1 + np.exp(-x))
    return sigmoid_x * (1 + x * (1 - sigmoid_x))
```

## 2. 其他常用激活函数

### 2.1 Sigmoid
**公式**: $f(x) = \frac{1}{1 + e^{-x}}$

**特点**:
- 输出范围 (0, 1)，适合二分类输出层
- 平滑可导
- 缺点：容易梯度消失，输出非零中心

**实现**:
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)
```

### 2.2 Tanh (Hyperbolic Tangent)
**公式**: $f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

**特点**:
- 输出范围 (-1, 1)，零中心化
- 比 Sigmoid 梯度更强
- 仍存在梯度消失问题

**实现**:
```python
def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2
```

### 2.3 Softmax
**公式**: $f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$

**特点**:
- 多分类问题的标准输出层激活函数
- 将输出转换为概率分布（和为 1）
- 通常与交叉熵损失函数配合使用

**实现**:
```python
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # 数值稳定性
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)
```

### 2.4 Softplus
**公式**: $f(x) = \ln(1 + e^x)$

**特点**:
- ReLU 的平滑版本
- 处处可导，无"死神经元"问题
- 计算成本高于 ReLU

**实现**:
```python
def softplus(x):
    return np.log(1 + np.exp(x))

def softplus_derivative(x):
    return 1 / (1 + np.exp(-x))  # 即 sigmoid
```

## 3. 激活函数选择建议

### 隐藏层
- **ReLU**: 最常用，适合大多数情况
- **Leaky ReLU / PReLU**: 当遇到"死神经元"问题时使用
- **GELU / Swish**: 在 Transformer 等现代架构中表现优异
- **ELU**: 需要更好的归一化特性时使用

### 输出层
- **Sigmoid**: 二分类问题
- **Softmax**: 多分类问题
- **线性/无激活**: 回归问题

## 4. 总结

| 激活函数 | 优点 | 缺点 | 适用场景 |
|---------|------|------|---------|
| ReLU | 计算简单，梯度稳定 | 死神经元问题 | 隐藏层（最常用） |
| Leaky ReLU | 解决死神经元 | 需要调参 $\alpha$ | 隐藏层 |
| ELU | 平滑，归一化好 | 计算成本高 | 隐藏层 |
| GELU | 现代架构表现好 | 计算复杂 | Transformer 等 |
| Sigmoid | 输出概率 | 梯度消失 | 输出层（二分类） |
| Tanh | 零中心化 | 梯度消失 | 隐藏层（较少用） |
| Softmax | 多分类标准 | 仅用于输出层 | 输出层（多分类） |
